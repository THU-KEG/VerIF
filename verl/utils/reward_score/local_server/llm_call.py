import re
import random
import requests
import sys
sys.path.append('/mnt/ph/ScaleIF/verl/verl/utils/reward_score/local_server')
from build_model import APIModel
# api_url="http://172.18.199.104:8000/v1/chat/completions"
# model_name="Qwen2.5-72B-Instruct"
# api_url="http://172.18.200.150:8000/v1/chat/completions"
# model_name="default"

api_url="http://172.20.77.43:8000/v1/chat/completions"
model_name="QwQ-32B"
# api_model = APIModel("http://172.18.201.204:8008/v1", "Qwen2.5-72B-Instruct")
api_model = APIModel("http://172.20.76.190:8008/v1", "DeepSeek-R1-Distill-Qwen-7B")


def generate_chat(messages, max_tokens=128, temperature=0.0):
    # response = api_model.generate_chat(messages, max_tokens, temperature)
    # return response.strip()
    request_data = {
        "model": model_name,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature
    }
    response = requests.post(
        api_url,
        json=request_data,
        headers={"Content-Type": "application/json"},
        timeout=900 
    )
    if response.status_code == 200:
        resp_json = response.json()
        content = resp_json['choices'][0]['message']['content'].strip()
        return content
    else:
        print(
            f"Failed to fetch response: {response.status_code}, {response.text}"
        )
        return None


# 

def extract_chat(messages, max_tokens=128, temperature=0.0):
    response = api_model.generate_chat(messages, max_tokens, temperature)
    return response.strip()

    # request_data = {
    #     "model": "Qwen2.5-72B-Instruct",
    #     "messages": messages,
    #     "max_tokens": max_tokens,
    #     "temperature": temperature
    # }
    # response = requests.post(
    #     "http://172.18.198.222:8000/v1/chat/completions",
    #     json=request_data,
    #     headers={"Content-Type": "application/json"},
    #     timeout=900 
    # )
    # if response.status_code == 200:
    #     resp_json = response.json()
    #     content = resp_json['choices'][0]['message']['content'].strip()
    #     return content
    # else:
    #     print(
    #         f"Failed to fetch response: {response.status_code}, {response.text}"
    #     )
    #     return None
    

prompt_template = """
è¯·åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬æ˜¯å¦æ»¡è¶³ç»™å®šçš„çº¦æŸï¼Œä»…å›ç­”æ˜¯æˆ–å¦ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚


åŸå§‹æŒ‡ä»¤ï¼š$I$

æ–‡æœ¬ï¼š$R$

çº¦æŸï¼š$C$

åŸå§‹æŒ‡ä»¤æè¿°äº†åŸºæœ¬çš„ä»»åŠ¡ä¿¡æ¯ï¼Œç»™å®šçš„çº¦æŸä»‹ç»äº†åº”è¯¥æ»¡è¶³çš„å…·ä½“çš„ä¸€ä¸ªçº¦æŸã€‚
è¯·åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬æ˜¯å¦æ»¡è¶³ç»™å®šçš„è¿™ä¸ªçº¦æŸï¼ˆä»…ä»…åˆ¤æ–­æ˜¯å¦æ»¡è¶³ç»™å®šçš„çº¦æŸï¼‰ï¼Œä»…å›ç­”æ˜¯æˆ–å¦ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚
"""


def llm_judge(instruction, response, constraint):
    if isinstance(response, list):
        response = "\n\n".join(response)
    prompt = prompt_template.replace("$R$", response).replace("$C$", constraint).replace("$I", instruction)
    data = [
        {"role": "user", "content": prompt}
    ]
    response = generate_chat(data)

    return response[0] == "æ˜¯"
    # score = extract_score(response)
    # return score
    # chinese_chars = re.findall(r'[\u4e00-\u9fff\U00020000-\U0002EBEF]', response)


def llm_extract(instruction, response, specific_prompt):
    prompt_suffix = "\nè¯·ç›´æ¥è¾“å‡ºæ–‡æœ¬ä¸­çš„åŸæ–‡ä¿¡æ¯ï¼Œä¸è¦æ”¹å†™ï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„ä¿¡æ¯ã€‚"

    prompt = f"æ–‡æœ¬ï¼š{response}\næŠ½å–è¦æ±‚ï¼š{specific_prompt}" + prompt_suffix
    data = [
        {"role": "user", "content": prompt}
    ]
    response = extract_chat(data, max_tokens=1024)
    return response


def extract_score(text):
    match = re.search(r'\[\[(\d+)\]\]', text)
    try:
        return int(match.group(1))
    except:
        return 0


# def llm_score(instruction, response):
#     prompt = f"""
#     è¯·è€ƒè™‘ä»¥ä¸‹4ä¸ªç»´åº¦å¯¹æ¨¡å‹çš„å›å¤è¿›è¡Œè¯„åˆ†ï¼š
#     1. æµç•…æ€§ï¼ˆFluencyï¼‰ï¼šè¾“å‡ºæ˜¯å¦ç¬¦åˆè‡ªç„¶è¯­è¨€è¡¨è¾¾ä¹ æƒ¯ï¼Œæ˜¯å¦é€šé¡ºã€æ— è¯­æ³•é”™è¯¯ã€‚
#     2. å‡†ç¡®æ€§ï¼ˆAccuracyï¼‰ï¼šè¾“å‡ºå†…å®¹æ˜¯å¦ä¸äº‹å®æˆ–è¾“å…¥ä¿¡æ¯ä¸€è‡´ï¼Œæ˜¯å¦å­˜åœ¨é”™è¯¯æˆ–åå·®ã€‚
#     3. ç›¸å…³æ€§ï¼ˆRelevanceï¼‰ï¼šè¾“å‡ºæ˜¯å¦ä¸è¾“å…¥é—®é¢˜æˆ–ä»»åŠ¡ç´§å¯†ç›¸å…³ï¼Œæ˜¯å¦åŒ…å«æ— å…³ä¿¡æ¯ã€‚
#     4. å¤šæ ·æ€§ï¼ˆDiversityï¼‰/ ä¿¡æ¯é‡ï¼ˆInformativenessï¼‰ï¼šè¾“å‡ºæ˜¯å¦åŒ…å«ä¸°å¯Œçš„ä¿¡æ¯ï¼Œè¡¨è¾¾æ–¹å¼æ˜¯å¦å¤šæ ·ï¼Œé¿å…é‡å¤æˆ–åˆ»æ¿çš„å›ç­”ã€‚
    
#     [æŒ‡ä»¤]
#     {instruction}

#     [å›å¤]
#     {response}
    
#     è¯·ä»æµç•…æ€§ã€å‡†ç¡®æ€§ã€ç›¸å…³æ€§ã€å¤šæ ·æ€§4ä¸ªç»´åº¦å¯¹å›å¤è¿›è¡Œè¯„åˆ†ï¼Œè¦æ±‚è¾“å‡º1-10ä¸­çš„ä¸€ä¸ªæ•´æ•°ï¼Œåªç”¨è¾“å‡ºä¸€ä¸ªæ€»åˆ†å³å¯ã€‚
#     è¯·åœ¨å›ç­”çš„æœ€å¼€å§‹ç”¨[[score]]æ ¼å¼è¾“å‡ºä½ çš„åˆ†æ•°ã€‚
#     """
#     data = [
#         {"role": "user", "content": prompt}
#     ]
#     response = generate_chat(data, max_tokens=128)
#     score = extract_score(response)
#     return score

def llm_score(instruction, response, checkers):
    prompt = f"""
    è¯·åˆ¤æ–­ç»™å®šçš„å›å¤æ˜¯å¦éµå¾ªæŒ‡ä»¤ä¸­çš„çº¦æŸï¼Œæ¯”å¦‚é•¿åº¦ã€é£æ ¼ã€æ ¼å¼ç­‰çº¦æŸã€‚
    
    [æŒ‡ä»¤]
    {instruction}

    [å›å¤]
    {response}

    [çº¦æŸ]
    {checkers}

    è¯·åˆ¤æ–­ç»™å®šçš„å›å¤æ˜¯å¦éµå¾ªæŒ‡ä»¤ä¸­çš„çº¦æŸï¼Œæ¯”å¦‚é•¿åº¦ã€é£æ ¼ã€æ ¼å¼ç­‰çº¦æŸã€‚
    è¯·åœ¨å›ç­”çš„æœ€å¼€å§‹ç”¨[[score]]æ ¼å¼è¾“å‡ºä½ çš„åˆ†æ•°ã€‚
    å¦‚æœéµå¾ªæ‰€æœ‰çš„çº¦æŸï¼Œè¯·è¾“å‡º[[1]]ï¼Œå¦åˆ™è¾“å‡º[[0]]
    """
    data = [
        {"role": "user", "content": prompt}
    ]
    response = generate_chat(data, max_tokens=4096)
    score = extract_score(response)
    return score



if __name__ == "__main__":
    # call_llm(
    #     "Why do some materials feel soft and others feel coarse? Provide the explanation with a touch of humor, and consider how texture affects usability in clothing and furniture design.The response should be formatted as a numbered list with each point providing a reason or explanation for why some materials feel soft and others feel coarse, include the keywords 'texture' and 'sensation'.",
    #     "1. Grab your favorite Python list.  \n2. Use the magic word 'print'.  \n3. Place the list in parentheses like it's a VIP guest.  \n4. Watch as Python spills the beans (or elements).  \n5. Revel in your coding prowess!  \n\n```python\nprint(your_list)\n``",
    #     "Provide the response with a humorous tone"
    # )
    result = llm_judge(
        "What is the speed of light, and how does it compare to the speed of sound in a vacuum? Please answer with a tone of excitement and wonder.The word 'light' should appear at least 3 times, and your response should contain exactly 3 sentences.",
        "Oh, the speed of light is a mind-blowing marvel of the universe, traveling at a staggering 299,792,458 meters per second (m/s)! ğŸŒŸ In comparison, the speed of sound in a vacuum is non-existent because sound needs a medium to travel, whereas light races through the void with unparalleled grace and swiftness. Imagine the thrill of light zooming across the cosmos, effortlessly outpacing any sound, and illuminating the mysteries of space with its incredible speed!",
        "Your response should contain exactly 3 sentences",
    )
    print(result)

    


